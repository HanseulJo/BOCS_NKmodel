{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BO_on_NKmodel_game.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNqBlWeLiMJ00zNbPp2dt4I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanseulJo/BOCS_NKmodel/blob/master/BO_on_NKmodel_game.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjz8ayGLouPJ"
      },
      "source": [
        "**To play the game, run all the cells below. The cell you can play the game is the last one. Refer to the contents.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQnMazltSkT3"
      },
      "source": [
        "# NKmodel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq43AJ7CS5B5"
      },
      "source": [
        "\"\"\"\n",
        "Author: Hanseul Cho\n",
        "Date: 2021.07.21 (last updated)\n",
        "\"\"\"\n",
        "\n",
        "from os import stat_result\n",
        "import numpy as np\n",
        "import sys\n",
        "import itertools\n",
        "\n",
        "class NKmodel(object):\n",
        "    \"\"\"\n",
        "    NKmodel class. A single NK model.\n",
        "    Huge thanks to https://github.com/elplatt/nkmodel.git\n",
        "    \n",
        "    <PARAM>\n",
        "    N: the number of loci\n",
        "    K: the number of the other dependent loci for each locus (0 <= K <= N-1)\n",
        "    A: the number of states that each locus can have (e.g.: 2 for binary variables)\n",
        "    <attributes>\n",
        "    self.interdependence : interdependency matrix in 2D boolean numpy array.\n",
        "    self.contributions   : list of dict's - ith dict maps (tuple (x_i, x_i0, x_i1, ..., x_iK) of length K) |--> (contribution(=payoff) f_i(x))\n",
        "    \"\"\"\n",
        "    def __init__(self, N, K, A=2, interdependence=None, contributions=None, random_seeds=(None, None)):\n",
        "        assert 0 <= K <= N-1\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.A = A\n",
        "        self.loci = range(N)\n",
        "        if interdependence is None:\n",
        "            # randomly generated interdependence matrix\n",
        "            self.interdependence = np.full((N,N), False)\n",
        "            rng_state_dep = np.random.RandomState(seed=random_seeds[0])\n",
        "            for i in self.loci:\n",
        "                dependence = [i] + list(rng_state_dep.choice(list(set(self.loci) - set([i])), size=K, replace=False))\n",
        "                self.interdependence[i][dependence] = True\n",
        "        else:\n",
        "            self.interdependence = interdependence\n",
        "        if contributions is None:\n",
        "            self.contributions = [{} for _ in self.loci]\n",
        "            rng_state_ctrb = np.random.RandomState(seed=random_seeds[1])\n",
        "            for i in range(N):\n",
        "                for label in itertools.product(range(A), repeat=K+1):  # K+1 subcollection of loci values that effects the locus i\n",
        "                    self.contributions[i][label] = float(rng_state_ctrb.random())  # float [0, 1)\n",
        "        else:\n",
        "            self.contributions = contributions\n",
        "\n",
        "    def _calculate_ith_contribution(self, state, i):\n",
        "        assert i in self.loci\n",
        "        assert type(state) == np.ndarray\n",
        "        interdep = self.interdependence[i].copy()\n",
        "        interdep[i] = False\n",
        "        label = tuple([state[i]] + list(state[interdep]))  # the value of i-th locus should be the first entry of the 'label'.\n",
        "        return self.contributions[i][label]\n",
        "\n",
        "    def fitness_and_contributions(self, state, negative=False):\n",
        "        \"\"\"\n",
        "        Given a state(: a tuple/string of length N), \n",
        "        Return fitness value and a list of contributions of each loci.\n",
        "        \"\"\"\n",
        "        if type(state) == str:\n",
        "            state = np.array([int(state[i]) for i in range(self.N)])\n",
        "        else:\n",
        "            state = np.array(state)\n",
        "        ctrbs = [self._calculate_ith_contribution(state, i) for i in self.loci]\n",
        "        fitness_value = sum(ctrbs) / self.N  # normalized(averaged) fitness value.\n",
        "        if negative:\n",
        "            fitness_value = -fitness_value\n",
        "        return fitness_value, ctrbs\n",
        "    \n",
        "    def fitness(self, state, negative=False):\n",
        "        f, _ = self.fitness_and_contributions(state, negative=negative)\n",
        "        return f\n",
        "    \n",
        "    def evaluate(self, state, negative=False):\n",
        "        if state.ndim == 1:\n",
        "            state = np.expand_dims(state, axis=0)\n",
        "        assert state.shape[1] == self.N, state.shape[1]\n",
        "        return np.array([self.fitness(state[i], negative=negative) for i in range(state.shape[0])])\n",
        "    \n",
        "    def fitness_and_contrib_diff(self, state_prev, flip_ind, fitness_prev=None, ctrbs_prev=None, negative=False):\n",
        "        state_new = flip(state_prev, flip_ind)\n",
        "        depend_on_flip = self.interdependence[:,flip_ind]\n",
        "        \n",
        "        if fitness_prev is None or ctrbs_prev is None:\n",
        "            fitness_prev, ctrbs_prev = self.fitness_and_contributions(state_prev, negative=negative)\n",
        "\n",
        "        fitness_new = fitness_prev * self.N\n",
        "        ctrbs_diff = np.zeros(self.N)\n",
        "        for j in np.nonzero(depend_on_flip)[0]:\n",
        "\n",
        "            ctrb_new_j = self._calculate_ith_contribution(state_new, j)\n",
        "            ctrb_prev_j = ctrbs_prev[j]\n",
        "            fitness_new = fitness_new - ctrb_prev_j + ctrb_new_j\n",
        "            ctrbs_diff[j] = ctrb_new_j - ctrb_prev_j\n",
        "        fitness_new /= self.N\n",
        "\n",
        "        return fitness_new, ctrbs_diff\n",
        "\n",
        "\n",
        "    def landscape(self, negative=False):\n",
        "        \"\"\"\n",
        "        Return a dictionary mapping each state to its fitness value. (Naive algorithm)\n",
        "        \"\"\"\n",
        "        landscape_dic = {}\n",
        "        states = itertools.product(range(self.A), repeat=self.N)\n",
        "        for state in states:\n",
        "            landscape_dic[state] = self.fitness(state, negative=negative)\n",
        "        return landscape_dic\n",
        "\n",
        "    def landscape_with_contributions(self):\n",
        "        \"\"\"\n",
        "        Return a dictionary mapping each state to its fitness value and contributions of loci. (Naive algorithm)\n",
        "        \"\"\"\n",
        "        return {state: self.fitness_and_contributions(state) for state in itertools.product(range(self.A), repeat=self.N)}  # along all possible states\n",
        "\n",
        "    def get_global_optimum(self, negative=False, anti_opt=False, cache=False, given_landscape=None):\n",
        "        \"\"\"\n",
        "        Global maximum fitness value and its maximizer(state), in a NAIVE way.\n",
        "        If \"anti_opt=True\", this returns the \"minimum\" fitness value and \"minimizer\". \n",
        "        \"\"\"\n",
        "        landscape = self.landscape() if given_landscape is None else given_landscape\n",
        "        optimum = max(landscape.values()) if not anti_opt else min(landscape.values())\n",
        "        states = [s for s in landscape.keys() if landscape[s] == optimum]\n",
        "        if negative:\n",
        "            optimum = -optimum\n",
        "        if cache:\n",
        "            return optimum, states, landscape\n",
        "        else:\n",
        "            return optimum, states\n",
        "        \n",
        "    def get_optimum_and_more(self, order, negative=False, anti_opt=False, cache=False, given_landscape=None):\n",
        "        \"\"\"\n",
        "        First several maximum fitness values and their maximizers(states), in a NAIVE way.\n",
        "        If \"anti_opt=True\", this returns first several \"minimum\" fitness values and \"minimizers\". \n",
        "        \"\"\"\n",
        "        landscape = self.landscape() if given_landscape is None else given_landscape\n",
        "        landscape_list = sorted(landscape.items(), key=lambda x: -x[1])\n",
        "        if anti_opt:\n",
        "            landscape_list.reverse()\n",
        "        state_opt, fit_opt = landscape_list[0]\n",
        "        if negative:\n",
        "            fit_opt = -fit_opt\n",
        "        optima2states = [{\"fitness\": fit_opt, \"states\":[state_opt]}]\n",
        "        cnt = 1\n",
        "        for state, fitness in landscape_list[1:]:\n",
        "            if negative:\n",
        "                fitness = -fitness\n",
        "            if fitness == optima2states[-1][\"fitness\"]:\n",
        "                optima2states[-1][\"states\"].append(state)\n",
        "            else:\n",
        "                cnt += 1\n",
        "                if cnt > order:\n",
        "                    break\n",
        "                optima2states.append({\"fitness\": fitness, \"states\":[state]})\n",
        "        if cache:\n",
        "            return optima2states, landscape\n",
        "        else:\n",
        "            return optima2states\n",
        "    \n",
        "    def print_info(self, path=None):\n",
        "        order = min(10, 2**self.N)\n",
        "        optlist = self.get_optimum_and_more(order)\n",
        "        if path is None:\n",
        "            print(\"\\nInterdependence Matrix:\")\n",
        "            for i in range(self.N):\n",
        "                print(\"\".join([\"X\" if b else \"O\" for b in self.interdependence[i]]))\n",
        "            print(\"\\nLandscape:\")\n",
        "            d = self.landscape_with_contributions()\n",
        "            for state, (fit, ctrbs) in d.items():\n",
        "                ctrbs = [str(round(v, 4)) for v in ctrbs]\n",
        "                fit = str(round(fit, 4))\n",
        "                state = \"\".join([str(x) for x in state])\n",
        "                print(\"\\t\".join([state] + ctrbs + [fit]))\n",
        "            for i in range(order):\n",
        "                opt, optstates = optlist[i][\"fitness\"], optlist[i][\"states\"]\n",
        "                print(f\"{i+1}-th optimum: {opt} {optstates}\")\n",
        "        else:\n",
        "            with open(path + \"/knowledge.txt\", \"w\") as f1:\n",
        "                for i in range(self.N):\n",
        "                    print(\"\".join([\"X\" if b else \"O\" for b in self.interdependence[i]]), file=f1)\n",
        "            with open(path + \"/landscape.txt\", \"w\") as f2:\n",
        "                d = self.landscape_with_contributions()\n",
        "                for state, (fit, ctrbs) in d.items():\n",
        "                    ctrbs = [str(round(v, 4)) for v in ctrbs]\n",
        "                    fit = str(round(fit, 4))\n",
        "                    state = \"\".join([str(x) for x in state])\n",
        "                    print(\"\\t\".join([state] + ctrbs + [fit]), file=f2)\n",
        "            with open(path + \"/rankboard.txt\", \"w\") as f3:\n",
        "                for i in range(order):\n",
        "                    opt, optstates = optlist[i][\"fitness\"], optlist[i][\"states\"]\n",
        "                    print(f\"{i+1}-th optimum: {opt} {optstates}\", file=f3)\n",
        "\n",
        "    def rank_by_fitness(self, fitness_value, given_landscape=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "def _generate_random_seeds(seed_str, n_im_seed=3, n_ctrbs_seed=3, n_init_point_seed=3):\n",
        "    \"\"\"\n",
        "    Original code: COMBO.experiments.random_seed_config.py\n",
        "    \"\"\"\n",
        "    rng_state = np.random.RandomState(seed=sum([ord(ch) for ch in seed_str]))\n",
        "    result = {}\n",
        "    for _ in range(n_im_seed):\n",
        "        result[rng_state.randint(0, 10000)] = (list(rng_state.randint(0, 10000, (n_ctrbs_seed,))), list(rng_state.randint(0, 10000, (n_init_point_seed,))))\n",
        "    return result\n",
        "\n",
        "def generate_random_seeds_nkmodel():\n",
        "    \"\"\"\n",
        "    Original code: COMBO.experiments.random_seed_config.py\n",
        "    \"\"\"\n",
        "    return _generate_random_seeds(seed_str=\"NK_MODEL\", n_im_seed=100, n_ctrbs_seed=100, n_init_point_seed=100)\n",
        "\n",
        "# Helper functions\n",
        "def flip(state, ind):\n",
        "    \"\"\" given a state (in numpy array), flip (0 <-> 1) a digit \"\"\"\n",
        "    assert state.ndim == 1, state.ndim\n",
        "    new_state = state.copy()\n",
        "    new_state[ind] = 1-new_state[ind]\n",
        "    return new_state\n",
        "\n",
        "def neighbors(state):\n",
        "    \"\"\" given a state (in numpy array), return a 2D array whose rows are neighbors of the state \"\"\"\n",
        "    return np.stack([flip(state, i) for i in range(len(state))])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-6VMtHqSoju"
      },
      "source": [
        "#LinReg, bhs (reference: BOCS github)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnUQIavsTAbj"
      },
      "source": [
        "# Author: Ricardo Baptista and Matthias Poloczek\n",
        "# Date:   June 2018\n",
        "#\n",
        "# See LICENSE.md for copyright information\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def bhs(Xorg, yorg, nsamples, burnin, thin):\n",
        "    # Implementation of the Bayesian horseshoe linear regression hierarchy.\n",
        "    # Parameters:\n",
        "    #   Xorg     = regressor matrix [n x p]\n",
        "    #   yorg     = response vector  [n x 1]\n",
        "    #   nsamples = number of samples for the Gibbs sampler (nsamples > 0)\n",
        "    #   burnin   = number of burnin (burnin >= 0)\n",
        "    #   thin     = thinning (thin >= 1)\n",
        "    #\n",
        "    # Returns:\n",
        "    #   beta     = regression parameters  [p x nsamples]\n",
        "    #   b0       = regression param. for constant [1 x nsamples]\n",
        "    #   s2       = noise variance sigma^2 [1 x nsamples]\n",
        "    #   t2       = hypervariance tau^2    [1 x nsamples]\n",
        "    #   l2       = hypervariance lambda^2 [p x nsamples]\n",
        "    #  \n",
        "    #\n",
        "    # Example:\n",
        "    # % Load a dataset:\n",
        "    # load hald;        \n",
        "    # % Run horseshoe sampler. Normalising the data is not required.\n",
        "    # [beta, b0] = bhs(ingredients, heat, 1000, 100, 10);    \n",
        "    # % Plot the samples of the regression coefficients:\n",
        "    # boxplot(beta', 'labels', {'tricalcium aluminate','tricalcium silicate',...\n",
        "    #   'tetracalcium aluminoferrite', 'beta-dicalcium silicate'});\n",
        "    # title('Bayesian linear regression with the horseshoe hierarchy');\n",
        "    # xlabel('Predictors');\n",
        "    # ylabel('Beta');\n",
        "    # grid;\n",
        "    #\n",
        "    #\n",
        "    # References:\n",
        "    # A simple sampler for the horseshoe estimator\n",
        "    # E. Makalic and D. F. Schmidt\n",
        "    # arXiv:1508.03884, 2015\n",
        "    #\n",
        "    # The horseshoe estimator for sparse signals\n",
        "    # C. M. Carvalho, N. G. Polson and J. G. Scott\n",
        "    # Biometrika, Vol. 97, No. 2, pp. 465--480, 2010\n",
        "    #\n",
        "    # (c) Copyright Enes Makalic and Daniel F. Schmidt, 2015\n",
        "    # Adapted to python by Ricardo Baptista, 2018\n",
        "\n",
        "    n, p = Xorg.shape\n",
        "\n",
        "    # Normalize data\n",
        "    X, _, _, y, muY = standardise(Xorg, yorg)\n",
        "\n",
        "    # Return values\n",
        "    beta = np.zeros((p, nsamples))\n",
        "    s2 = np.zeros((1, nsamples))\n",
        "    t2 = np.zeros((1, nsamples))\n",
        "    l2 = np.zeros((p, nsamples))\n",
        "\n",
        "    # Initial values\n",
        "    sigma2  = 1.\n",
        "    lambda2 = np.random.uniform(size=p)\n",
        "    tau2    = 1.\n",
        "    nu      = np.ones(p)\n",
        "    xi      = 1.\n",
        "\n",
        "    # pre-compute X'*X (used with fastmvg_rue)\n",
        "    XtX = np.matmul(X.T,X)  \n",
        "\n",
        "    # Gibbs sampler\n",
        "    k = 0\n",
        "    iter = 0\n",
        "    while(k < nsamples):\n",
        "\n",
        "        # Sample from the conditional posterior distribution\n",
        "        sigma = np.sqrt(sigma2)\n",
        "        Lambda_star = tau2 * np.diag(lambda2)\n",
        "        # Determine best sampler for conditional posterior of beta's\n",
        "        if (p > n) and (p > 200):\n",
        "            b = fastmvg(X/sigma, y/sigma, sigma2*Lambda_star)\n",
        "        else:\n",
        "            b = fastmvg_rue(X/sigma, XtX/sigma2, y/sigma, sigma2*Lambda_star)\n",
        "\n",
        "        # Sample sigma2\n",
        "        e = y - np.dot(X,b)\n",
        "        shape = (n + p) / 2.\n",
        "        scale = np.dot(e.T,e)/2. + np.sum(b**2/lambda2)/tau2/2.\n",
        "        sigma2 = 1. / np.random.gamma(shape, 1./scale)\n",
        "\n",
        "        # Sample lambda2\n",
        "        scale = 1./nu + b**2./2./tau2/sigma2\n",
        "        lambda2 = 1. / np.random.exponential(1./scale)\n",
        "\n",
        "        # Sample tau2\n",
        "        shape = (p + 1.)/2.\n",
        "        scale = 1./xi + np.sum(b**2./lambda2)/2./sigma2\n",
        "        tau2 = 1. / np.random.gamma(shape, 1./scale)\n",
        "\n",
        "        # Sample nu\n",
        "        scale = 1. + 1./lambda2\n",
        "        nu = 1. / np.random.exponential(1./scale)\n",
        "\n",
        "        # Sample xi\n",
        "        scale = 1. + 1./tau2\n",
        "        xi = 1. / np.random.exponential(1./scale)\n",
        "\n",
        "        # Store samples\n",
        "        iter = iter + 1;\n",
        "        if iter > burnin:\n",
        "            # thinning\n",
        "            if (iter % thin) == 0:\n",
        "                beta[:,k] = b\n",
        "                s2[:,k]   = sigma2\n",
        "                t2[:,k]   = tau2\n",
        "                l2[:,k]   = lambda2\n",
        "                k         = k + 1\n",
        "\n",
        "    # Re-scale coefficients\n",
        "    #div_vector = np.vectorize(np.divide)\n",
        "    #beta = div_vector(beta.T, normX)\n",
        "    #b0 = muY-np.dot(muX,beta)\n",
        "    b0 = muY\n",
        "\n",
        "    return (beta, b0, s2, t2, l2)\n",
        "\n",
        "def fastmvg(Phi, alpha, D):\n",
        "    # Fast sampler for multivariate Gaussian distributions (large p, p > n) of \n",
        "    #  the form N(mu, S), where\n",
        "    #       mu = S Phi' y\n",
        "    #       S  = inv(Phi'Phi + inv(D))\n",
        "    # Reference: \n",
        "    #   Fast sampling with Gaussian scale-mixture priors in high-dimensional \n",
        "    #   regression, A. Bhattacharya, A. Chakraborty and B. K. Mallick\n",
        "    #   arXiv:1506.04778\n",
        "\n",
        "    n, p = Phi.shape\n",
        "\n",
        "    d = np.diag(D)\n",
        "    u = np.random.randn(p) * np.sqrt(d)\n",
        "    delta = np.random.randn(n)\n",
        "    v = np.dot(Phi,u) + delta\n",
        "    #w = np.linalg.solve(np.matmul(np.matmul(Phi,D),Phi.T) + np.eye(n), alpha - v)\n",
        "    #x = u + np.dot(D,np.dot(Phi.T,w))\n",
        "    mult_vector = np.vectorize(np.multiply)\n",
        "    Dpt = mult_vector(Phi.T, d[:,np.newaxis])\n",
        "    w = np.linalg.solve(np.matmul(Phi,Dpt) + np.eye(n), alpha - v)\n",
        "    x = u + np.dot(Dpt,w)\n",
        "\n",
        "    return x\n",
        "\n",
        "def fastmvg_rue(Phi, PtP, alpha, D):\n",
        "    # Another sampler for multivariate Gaussians (small p) of the form\n",
        "    #  N(mu, S), where\n",
        "    #  mu = S Phi' y\n",
        "    #  S  = inv(Phi'Phi + inv(D))\n",
        "    #\n",
        "    # Here, PtP = Phi'*Phi (X'X is precomputed)\n",
        "    #\n",
        "    # Reference:\n",
        "    #   Rue, H. (2001). Fast sampling of gaussian markov random fields. Journal\n",
        "    #   of the Royal Statistical Society: Series B (Statistical Methodology) \n",
        "    #   63, 325-338\n",
        "\n",
        "    p = Phi.shape[1]\n",
        "    Dinv = np.diag(1./np.diag(D))\n",
        "\n",
        "    # regularize PtP + Dinv matrix for small negative eigenvalues\n",
        "    try:\n",
        "        L = np.linalg.cholesky(PtP + Dinv)\n",
        "    except:\n",
        "        mat  = PtP + Dinv\n",
        "        Smat = (mat + mat.T)/2.\n",
        "        maxEig_Smat = np.max(np.linalg.eigvals(Smat))\n",
        "        L = np.linalg.cholesky(Smat + maxEig_Smat*1e-15*np.eye(Smat.shape[0]))\n",
        "\n",
        "    v = np.linalg.solve(L, np.dot(Phi.T,alpha))\n",
        "    m = np.linalg.solve(L.T, v)\n",
        "    w = np.linalg.solve(L.T, np.random.randn(p))\n",
        "\n",
        "    x = m + w\n",
        "\n",
        "    return x\n",
        "\n",
        "def standardise(X, y):\n",
        "    # Standardize the covariates to have zero mean and x_i'x_i = 1\n",
        "\n",
        "    # set params\n",
        "    n = X.shape[0]\n",
        "    meanX = np.mean(X, axis=0)\n",
        "    stdX  = np.std(X, axis=0) * np.sqrt(n)\n",
        "\n",
        "    # Standardize X's\n",
        "    #sub_vector = np.vectorize(np.subtract)\n",
        "    #X = sub_vector(X, meanX)\n",
        "    #div_vector = np.vectorize(np.divide)\n",
        "    #X = div_vector(X, stdX)\n",
        "\n",
        "    # Standardize y's\n",
        "    meany = np.mean(y)\n",
        "    y = y - meany\n",
        "\n",
        "    return (X, meanX, stdX, y, meany)\n",
        "\n",
        "# -- END OF FILE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JYO5pk4TG8F"
      },
      "source": [
        "# Author: Ricardo Baptista and Matthias Poloczek\n",
        "# Date:   June 2018\n",
        "#\n",
        "# See LICENSE.md for copyright information\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from sklearn.preprocessing import PolynomialFeatures # added by HS\n",
        "\n",
        "class LinReg:\n",
        "\n",
        "\tdef __init__(self, nVars, order):\n",
        "\t\tself.nVars = nVars\n",
        "\t\tself.order = order\n",
        "\n",
        "\t# ---------------------------------------------------------\n",
        "\t# ---------------------------------------------------------\n",
        "\n",
        "\tdef setupData(self):\n",
        "\n",
        "\t\t# limit data to unique points\n",
        "\t\tX, x_idx = np.unique(self.xTrain, axis=0, return_index=True)\n",
        "\t\ty = self.yTrain[x_idx]\n",
        "\n",
        "\t\t# set upper threshold\n",
        "\t\tinfT = 1e6\n",
        "\n",
        "\t\t# separate samples based on Inf output\n",
        "\t\ty_Infidx  = np.where(np.abs(y) > infT)[0]\n",
        "\t\ty_nInfidx = np.setdiff1d(np.arange(len(y)), y_Infidx)\n",
        "\n",
        "\t\t# save samples in two sets of variables\n",
        "\t\tself.xInf = X[y_Infidx,:]\n",
        "\t\tself.yInf = y[y_Infidx]\n",
        "\n",
        "\t\tself.xTrain = X[y_nInfidx,:]\n",
        "\t\tself.yTrain = y[y_nInfidx]\n",
        "\n",
        "\t# ---------------------------------------------------------\n",
        "\t# ---------------------------------------------------------\n",
        "\n",
        "\tdef train(self, inputs):\n",
        "\n",
        "\t\t# Set nGibbs (Gibbs iterations to run)\n",
        "\t\tnGibbs = int(1e3)\n",
        "\n",
        "\t\t# set data\n",
        "\t\tself.xTrain = inputs['x_vals']\n",
        "\t\tself.yTrain = inputs['y_vals']\n",
        "\n",
        "\t\t# setup data for training\n",
        "\t\tself.setupData()\n",
        "\n",
        "\t\t# create matrix with all covariates based on order\n",
        "\t\tself.xTrain = self.order_effects(self.xTrain, self.order)\n",
        "\t\t(nSamps, nCoeffs) = self.xTrain.shape\n",
        "\n",
        "\t\t# check if x_train contains columns with zeros or duplicates\n",
        "\t\t# and find the corresponding indices\n",
        "\t\tcheck_zero = np.all(self.xTrain == np.zeros((nSamps,1)), axis=0)\n",
        "\t\tidx_zero   = np.where(check_zero == True)[0]\n",
        "\t\tidx_nnzero = np.where(check_zero == False)[0]\n",
        "\n",
        "\t\t# remove columns of zeros in self.xTrain\n",
        "\t\tif np.any(check_zero):\n",
        "\t\t\tself.xTrain = self.xTrain[:,idx_nnzero]\n",
        "\n",
        "\t\t# run Gibbs sampler for nGibbs steps\n",
        "\t\tattempt = 1\n",
        "\t\ttrial = 20\n",
        "\t\twhile(attempt):\n",
        "\n",
        "\t\t\t# re-run if there is an error during sampling\n",
        "\t\t\ttry:\n",
        "\t\t\t\talphaGibbs,a0,_,_,_ = bhs(self.xTrain,self.yTrain,nGibbs,0,1)\n",
        "\t\t\texcept:\n",
        "\t\t\t\tprint('error during Gibbs sampling. Trying again.')\n",
        "\t\t\t\ttrial -= 1\n",
        "\t\t\t\tif trial > 0:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\traise TimeoutError\n",
        "\n",
        "\t\t\t# run until alpha matrix does not contain any NaNs\n",
        "\t\t\tif not np.isnan(alphaGibbs).any():\n",
        "\t\t\t\tattempt = 0\n",
        "\t\t\t\n",
        "\t\t# append zeros back - note alpha(1,:) is linear intercept\n",
        "\t\talpha_pad = np.zeros(nCoeffs)\n",
        "\t\talpha_pad[idx_nnzero] = alphaGibbs[:,-1]\n",
        "\t\tself.alpha = np.append(a0, alpha_pad)\n",
        "\n",
        "\t# ---------------------------------------------------------\n",
        "\t# ---------------------------------------------------------\n",
        "\n",
        "\tdef surrogate_model(self, x, alpha):\n",
        "\t\t# SURROGATE_MODEL: Function evaluates the linear model\n",
        "\t\t# Assumption: input x only contains one row -- loosen by HS\n",
        "\n",
        "\t\t# generate x_all (all basis vectors) based on model order\n",
        "\t\t#x_all = np.append(1, self.order_effects(x, self.order))\n",
        "\t\tx_all = PolynomialFeatures(degree=self.order, interaction_only=True).fit_transform(x)\n",
        "\n",
        "\t\t# check if x maps to an Inf output (if so, barrier=Inf)\n",
        "\t\tbarrier = 0.\n",
        "\t\tif self.xInf.shape[0] != 0:\n",
        "\t\t\tif np.equal(x, self.xInf).all(axis=1).any():\n",
        "\t\t\t\tbarrier = np.inf\n",
        "\n",
        "\t\t# compute and return objective with barrier\n",
        "\t\tout = x_all @ alpha.reshape((-1,1)) + barrier\n",
        "\t\t\n",
        "\t\treturn out.reshape(-1)\n",
        "\n",
        "\t# ---------------------------------------------------------\n",
        "\t# ---------------------------------------------------------\n",
        "\n",
        "\tdef order_effects(self, x_vals, ord_t):\n",
        "\t\t# order_effects: Function computes data matrix for all coupling\n",
        "\t\t# orders to be added into linear regression model.\n",
        "\n",
        "\t\t# Find number of variables\n",
        "\t\tn_samp, n_vars = x_vals.shape\n",
        "\n",
        "\t\t# Generate matrix to store results\n",
        "\t\tx_allpairs = x_vals\n",
        "\n",
        "\t\tfor ord_i in range(2,ord_t+1):\n",
        "\n",
        "\t\t\t# generate all combinations of indices (without diagonals)\n",
        "\t\t\toffdProd = np.array(list(combinations(np.arange(n_vars),ord_i)))\n",
        "\n",
        "\t\t\t# generate products of input variables\n",
        "\t\t\tx_comb = np.zeros((n_samp, offdProd.shape[0], ord_i))\n",
        "\t\t\tfor j in range(ord_i):\n",
        "\t\t\t\tx_comb[:,:,j] = x_vals[:,offdProd[:,j]]\n",
        "\t\t\tx_allpairs = np.append(x_allpairs, np.prod(x_comb,axis=2),axis=1)\n",
        "\n",
        "\t\treturn x_allpairs\n",
        "\n",
        "\n",
        "# -- END OF FILE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0NiMParSqek"
      },
      "source": [
        "# BOCS + NKmodel + Local Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCM3PNE1Em4c"
      },
      "source": [
        "## Initial settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HTSAVg3JjX9"
      },
      "source": [
        "import os, sys, time, itertools, random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from easydict import EasyDict\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#from LinReg import LinReg\n",
        "#from NKmodel import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT2yitYdJr2g"
      },
      "source": [
        "args = EasyDict()\n",
        "\n",
        "args.random_seed = 2021\n",
        "args.n_eval = 18\n",
        "args.n_init = 2\n",
        "args.N = 6\n",
        "args.K = 1\n",
        "args.A = 2\n",
        "args.terminal_size = 50\n",
        "\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "states = np.stack([np.array(tup) for tup in itertools.product(range(2), repeat=6)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUC-hDHYTcPM"
      },
      "source": [
        "# Miscelleneous Funtions for states (numpy.ndarray) ---> NKmodel.py\n",
        "# 수정: path, wander 함수: flip indices를 return하도록 바뀜.\n",
        "def hamming_dist(start, end):\n",
        "    assert start.ndim == end.ndim == 1 , (start.ndim, end.ndim)\n",
        "    assert start.size == end.size, (start.size, end.size)\n",
        "    return (start != end).sum()\n",
        "\n",
        "def is_reachable(start, end, flips):\n",
        "    return (start + end).sum() % 2 == flips % 2 and hamming_dist(start,end) <= flips\n",
        "\n",
        "def path(start, end, flips):\n",
        "    assert is_reachable(start, end, flips)\n",
        "    if flips == 0:\n",
        "        return np.array([[]])\n",
        "    x = start.copy()\n",
        "    flip_inds = np.arange(len(start))[start != end]\n",
        "    np.random.shuffle(flip_inds)\n",
        "    return flip_inds\n",
        "\n",
        "def wander(start, flips):\n",
        "    assert start.ndim == 1 and flips % 2 == 0\n",
        "    x = start.copy()\n",
        "    n = flips // 2\n",
        "    flip_inds = np.arange(start.shape[0])\n",
        "    np.random.shuffle(flip_inds)\n",
        "    flip_inds = np.array(list(flip_inds[:n]) + list(flip_inds[:n]))\n",
        "    return flip_inds\n",
        "\n",
        "def is_visited(x_vals, x_new):\n",
        "    assert x_new.ndim == 1, x_new.ndim\n",
        "    return np.all(x_new == x_vals, axis=1).any()\n",
        "        \n",
        "if False:\n",
        "    #example\n",
        "    print(hamming_dist(np.array([0,0,0,1,0,1]), np.array([0,0,1,0,1,1])))\n",
        "    print(is_reachable(np.array([0,0,0,1,0,1]), np.array([0,0,1,0,1,1]), 1))\n",
        "    print(is_reachable(np.array([0,0,0,1,0,1]), np.array([0,0,1,0,1,1]), 2))\n",
        "    print(is_reachable(np.array([0,0,0,1,0,1]), np.array([0,0,1,0,1,1]), 3))\n",
        "    print(is_reachable(np.array([0,0,0,1,0,1]), np.array([0,0,1,0,1,1]), 4))\n",
        "    print(is_reachable(np.array([0,0,0,1,0,1]), np.array([0,0,1,0,1,1]), 5))\n",
        "    print(path(np.array([0,0,0,1,0,1]), np.array([0,0,1,0,1,1]), hamming_dist(np.array([0,0,0,1,0,1]), np.array([0,0,1,0,1,1]))))\n",
        "    print(wander(np.array([0,0,0,1,0,1]), 2))\n",
        "    print(wander(np.array([0,0,0,1,0,1]), 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKHIazX8vZ2x"
      },
      "source": [
        "# add new data\n",
        "def add_data(inputs, x, y):\n",
        "  assert len(y) == 1\n",
        "  inputs['x_vals'] = np.vstack((inputs['x_vals'], x))\n",
        "  inputs['y_vals'] = np.hstack((inputs['y_vals'], y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoAKTFbKteWw"
      },
      "source": [
        "def Construct_NKmodel(args, im_seed_num=None, ctrbs_seed_num=None, verbose=False):\n",
        "\n",
        "    if im_seed_num is None:\n",
        "        im_seed_num = np.random.randint(100)\n",
        "    if ctrbs_seed_num is None:\n",
        "        ctrbs_seed_num = np.random.randint(100)\n",
        "    \n",
        "    # Random Seed for NKmodel\n",
        "    random_seeds = generate_random_seeds_nkmodel()\n",
        "    im_seed_ = sorted(random_seeds.keys())[im_seed_num]\n",
        "    ctrbs_seed_list_, _ = sorted(random_seeds[im_seed_])\n",
        "    ctrbs_seed_ = ctrbs_seed_list_[ctrbs_seed_num]\n",
        "\n",
        "    # Create NK model\n",
        "    nkmodel = NKmodel(args.N, args.K, A=args.A, random_seeds=(im_seed_, ctrbs_seed_))\n",
        "\n",
        "    # Miscelleneous\n",
        "    if verbose:\n",
        "        print(f\"im_seed_num {im_seed_num} ctrbs_seed_num {ctrbs_seed_num}\")\n",
        "\n",
        "    return nkmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIgBywqA5lqA"
      },
      "source": [
        "## 0. Random walk \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFD_XeC9w4c"
      },
      "source": [
        "# 수정: index 추천으로 바뀜\n",
        "def random_next(inputs, curr_x=None):  # avoid visited nbh as much as possible\n",
        "    if curr_x is None:\n",
        "        curr_x = inputs['x_vals'][-1]\n",
        "    num_nbh = curr_x.shape[0]  # N\n",
        "    nbh_inds = np.arange(num_nbh)\n",
        "    np.random.shuffle(nbh_inds)\n",
        "    i_ = 0\n",
        "    while i_ < num_nbh:\n",
        "        flip_ind = nbh_inds[i_]\n",
        "        if not is_visited(inputs['x_vals'], flip(curr_x.reshape(-1), flip_ind)):\n",
        "            break\n",
        "        else:\n",
        "            i_ += 1\n",
        "    if i_ == num_nbh:\n",
        "        flip_ind = np.random.randint(num_nbh)\n",
        "    return flip_ind\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrpdy15w-0qz"
      },
      "source": [
        "def monitor_reachable_best(inputs, t, n_eval):\n",
        "    curr_x = inputs['x_vals'][-1]\n",
        "    left_chance = n_eval - t\n",
        "    y_sort_ind = np.argsort(-inputs['y_vals'])  # positive: min -y ~= max fitness\n",
        "    PATH = None\n",
        "    for j in y_sort_ind:\n",
        "        objective_x = inputs['x_vals'][j]\n",
        "        objective_y = inputs['y_vals'][j]\n",
        "        if is_reachable(curr_x, objective_x, left_chance):\n",
        "            break\n",
        "    if hamming_dist(curr_x, objective_x) == left_chance:  # reachable in exactly 'left_chance' flips\n",
        "        PATH = path(curr_x, objective_x, left_chance)  # indices to flip to reach objective_x\n",
        "    return PATH, objective_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fABUySUP5hrh"
      },
      "source": [
        "def Random_Walk(args, inputs_, evaluate, progress_on=False, back_to_best=True):\n",
        "    if progress_on:\n",
        "        print(\"Random Walk\")\n",
        "        PROG_LEN = args.terminal_size \n",
        "        progress_bar = None\n",
        "    n_eval = args.n_eval\n",
        "    N = args.N\n",
        "\n",
        "    # copy inputs (containing initial state)\n",
        "    inputs = {k:v for k, v in inputs_.items()}\n",
        "    n_init = inputs['x_vals'].shape[0]\n",
        "\n",
        "    # randomly walk\n",
        "    next_x = inputs['x_vals'][-1].copy()\n",
        "    PATH = None\n",
        "    for t in range(n_init, n_eval+1):\n",
        "        # flip index suggestion and move a step\n",
        "        if PATH is None:\n",
        "            flip_ind = random_next(inputs, next_x)\n",
        "            next_x = flip(next_x.reshape(-1), flip_ind)\n",
        "            next_y = evaluate(next_x)\n",
        "            add_data(inputs, next_x, next_y)\n",
        "        elif back_to_best:\n",
        "            left_chance = n_eval - t + 1\n",
        "            flip_ind = PATH[-left_chance]\n",
        "            next_x = flip(next_x.reshape(-1), flip_ind)\n",
        "            next_y = evaluate(next_x)\n",
        "            add_data(inputs, next_x, next_y)\n",
        "            if left_chance-1>0 and (left_chance-1) % 2 == 0 and next_y > objective_y:  # new reachable best --> update PATH\n",
        "                PATH[-(left_chance-1):] = wander(next_x, left_chance-1)\n",
        "\n",
        "        if back_to_best and (PATH is None) and t >= n_eval - N:  # N is maximum possible hamming distance\n",
        "            PATH, objective_y = monitor_reachable_best(inputs, t, n_eval)\n",
        "        \n",
        "        if progress_on:\n",
        "            if progress_bar is not None:\n",
        "                print('\\b' * (len(progress_bar)+1))\n",
        "            ratio = t / n_eval\n",
        "            progressed_len = int(PROG_LEN * ratio)\n",
        "            progress_bar = f'Flip {t}/{n_eval} ({100.*ratio:.1f}%)' + '|' + '#'*progressed_len + '-'*(PROG_LEN-progressed_len) + '| '\n",
        "            progress_bar += f\"Latest: {inputs['y_vals'][-1]:.4f}, SoFarBest: {(inputs['y_vals'].max()):.4f}\"\n",
        "            sys.stdout.write(progress_bar)\n",
        "            sys.stdout.flush()\n",
        "            if t == n_eval:\n",
        "                print()\n",
        "    result = inputs['y_vals'][-1]\n",
        "    sofar_best = (inputs['y_vals']).max()\n",
        "    return result, sofar_best, inputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omonZP4aVv59"
      },
      "source": [
        "## 1. LinReg  (Surrogate model for BOCS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-uJIgqZdVoG"
      },
      "source": [
        "# trials 숫자에 대한 bottleneck 줄이는 법: 정해진 stat_model의 값으르 모든 state에 대해 다 구해놓고 필요할 때마다 불러와서 쓴다. --> calculated_acqs 도입\n",
        "def stochastic_ascent(stat_model, x_vals, max_flips, visit_weight, calculated_acqs=None):  # Name changed: descent --> ascent\n",
        "    assert max_flips > 0\n",
        "    x_vals = x_vals.copy()\n",
        "    x = x_vals[-1]\n",
        "    N = x.shape[0]\n",
        "\n",
        "    for flip_ in range(1,max_flips+1):\n",
        "        x_nbrs = neighbors(x)\n",
        "        \n",
        "        # evaluate acquisitions\n",
        "        if calculated_acqs is None:\n",
        "            nbrs_acquisition = np.array([stat_model(x_nbrs[i].reshape((1,-1))) for i in range(N)])\n",
        "        else:\n",
        "            nbrs_to_bin = [x_nbrs[i].dot(1 << np.arange(N)[::-1]) for i in range(N)]\n",
        "            nbrs_acquisition = np.array([calculated_acqs[b] for b in nbrs_to_bin])\n",
        "\n",
        "        # convert to probability (less acq, more prob)\n",
        "        m_ = nbrs_acquisition.min()\n",
        "        score = nbrs_acquisition.copy()\n",
        "        if m_ < 0:\n",
        "            score -= m_ # make all scores non negative\n",
        "        vw = visit_weight(max_flips+1-flip_)\n",
        "        for i in range(N):\n",
        "            if is_visited(x_vals, x_nbrs[i]):\n",
        "                score[i] *= vw  # give weight by a num <= 1 to visited vertex\n",
        "        score_stand = softmax(score*2)  # multiplying 3: just for appropriate scaling\n",
        "        #print(score_stand)\n",
        "\n",
        "        # update x\n",
        "        next_ind = int(np.random.choice(np.arange(N), p=score_stand))\n",
        "        x = x_nbrs[next_ind]\n",
        "        x_vals = np.concatenate((x_vals, x.reshape((1,-1))))\n",
        "\n",
        "    return nbrs_acquisition[next_ind]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKiXA_zYNHOI"
      },
      "source": [
        "# ver 2. nbr마다 ascent score 구하고, visited penalty (weight < 1)를 부과한 penalized score가 제일 높은 nbr 고름\n",
        "# visited penalty 는 max_flips 가 N=6 이하일 때는 1, 6보다 클 때는 클수록 weight -> 0\n",
        "# 수정: index 추천으로 바뀜\n",
        "# trials 숫자에 대한 bottleneck 줄이는 법: 정해진 stat_model의 값으르 모든 state에 대해 다 구해놓고 필요할 때마다 불러와서 쓴다.\n",
        "def greedy_neighbor_suggestion(args, stat_model, inputs, max_flips, trials=10, tqdm_on=True, calculated_acqs=None):\n",
        "    assert max_flips > 0\n",
        "\n",
        "    x_vals = inputs['x_vals'].copy()\n",
        "    x = x_vals[-1]\n",
        "    N = args.N\n",
        "    x_nbrs = neighbors(x)\n",
        "\n",
        "    # visited penalty\n",
        "    n_eval = args.n_eval\n",
        "    visit_weight = lambda fl: (n_eval - fl)/(n_eval - N) if fl>N else 1.\n",
        "    vw = visit_weight(max_flips)\n",
        "\n",
        "    if max_flips == 1:\n",
        "        if calculated_acqs is None:\n",
        "            best_nbr_ind = np.argmax(np.array([stat_model(x_nbrs[i].reshape((1,-1))) for i in range(N)]))\n",
        "        else:\n",
        "            nbrs_to_bin = [state_to_int(x_nbrs[i]) for i in range(N)]\n",
        "            best_nbr_ind = np.argmax(np.array([calculated_acqs[b] for b in nbrs_to_bin]))\n",
        "        return best_nbr_ind\n",
        "    elif max_flips > 1:\n",
        "        ascent_scores = []\n",
        "        it = tqdm(range(N), desc=f\"Greedy Search from {x}\") if tqdm_on else range(N)\n",
        "        for i in it:\n",
        "            if calculated_acqs is None:\n",
        "                _asc = [stochastic_ascent(stat_model, np.vstack((x_vals, x_nbrs[i])), max_flips-1, visit_weight) for _ in range(trials)]\n",
        "            else:\n",
        "                _asc = [stochastic_ascent(stat_model, np.vstack((x_vals, x_nbrs[i])), max_flips-1, visit_weight, calculated_acqs=calculated_acqs) for _ in range(trials)]\n",
        "            ascent_scores.append(sum(_asc)/trials)  # average\n",
        "        for i in range(N):\n",
        "            if is_visited(x_vals, x_nbrs[i]):\n",
        "                ascent_scores[i] *= vw  # give weight by a num <= 1\n",
        "        best_nbr_ind = np.argmax(np.array(ascent_scores))\n",
        "        return best_nbr_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rxww_0xrqvl"
      },
      "source": [
        "def BOCS_loc(args, inputs_, evaluate, tqdm_on=False, progress_on=False, back_to_best=True, ascent_trials=10, pre_calc_acq=False):\n",
        "    n_eval = args.n_eval\n",
        "    N = args.N\n",
        "\n",
        "    # copy inputs (containing initial state)\n",
        "    inputs = {k:v for k, v in inputs_.items()}\n",
        "    n_init = inputs['x_vals'].shape[0]\n",
        "\n",
        "    # Produce more initial states, by randomly walking\n",
        "    next_x = inputs['x_vals'][-1].copy()\n",
        "    for t in range(n_init, args.n_init):\n",
        "        flip_ind = random_next(inputs, next_x)\n",
        "        next_x = flip(next_x.reshape(-1), flip_ind)\n",
        "        next_y = evaluate(next_x)\n",
        "        add_data(inputs, next_x, next_y)\n",
        "    \n",
        "    # progress bar\n",
        "    if progress_on:\n",
        "        print(\"Local Search with BOCS\")\n",
        "        PROG_LEN = args.terminal_size \n",
        "        ratio = (args.n_init-1)/ n_eval\n",
        "        progressed_len = int(PROG_LEN * ratio)\n",
        "        progress_bar = f'Flip {args.n_init-1}/{n_eval} ({100.*ratio:.1f}%)' + '|' + '#'*progressed_len + '-'*(PROG_LEN-progressed_len) + '| '\n",
        "        progress_bar += f\"Latest: {inputs['y_vals'][-1]:.4f}, SoFarBest: {(inputs['y_vals'].min()):.4f}\"\n",
        "        sys.stdout.write(progress_bar)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    # Train initial statistical model\n",
        "    LR = LinReg(nVars=N, order=2)\n",
        "    LR.train(inputs)\n",
        "\n",
        "    PATH = None\n",
        "    for t in range(args.n_init, n_eval+1):\n",
        "        # flip index suggestion and move a step\n",
        "        if PATH is None:\n",
        "            stat_model = lambda x: LR.surrogate_model(x, LR.alpha)\n",
        "            if pre_calc_acq:\n",
        "                acqs_ = stat_model(states)\n",
        "                flip_ind = greedy_neighbor_suggestion(args, stat_model, inputs, n_eval - t + 1, tqdm_on=tqdm_on, trials=ascent_trials, calculated_acqs=acqs_)\n",
        "            else:\n",
        "                flip_ind = greedy_neighbor_suggestion(args, stat_model, inputs, n_eval - t + 1, tqdm_on=tqdm_on, trials=ascent_trials)\n",
        "            next_x = flip(next_x.reshape(-1), flip_ind)\n",
        "            next_y = evaluate(next_x)\n",
        "            add_data(inputs, next_x, next_y)\n",
        "        elif back_to_best:\n",
        "            left_chance = n_eval - t + 1\n",
        "            flip_ind = PATH[-left_chance]\n",
        "            next_x = flip(next_x.reshape(-1), flip_ind)\n",
        "            next_y = evaluate(next_x)\n",
        "            add_data(inputs, next_x, next_y)\n",
        "            if left_chance-1>0 and (left_chance-1) % 2 == 0 and next_y > objective_y:  # new reachable best --> update PATH\n",
        "                PATH[-(left_chance-1):] = wander(next_x, left_chance-1)\n",
        "        # train_surrogate model\n",
        "        LR.train(inputs)\n",
        "\n",
        "        if back_to_best and (PATH is None) and t >= n_eval - N:  # N is maximum possible hamming distance\n",
        "            PATH, objective_y = monitor_reachable_best(inputs, t, n_eval)\n",
        "\n",
        "        if progress_on:\n",
        "            if progress_bar is not None:\n",
        "                sys.stdout.write('\\b' * (len(progress_bar)))\n",
        "            ratio = t / n_eval\n",
        "            progressed_len = int(PROG_LEN * ratio)\n",
        "            progress_bar = f'Flip {t}/{n_eval} ({100.*ratio:.1f}%)' + '|' + '#'*progressed_len + '-'*(PROG_LEN-progressed_len) + '| '\n",
        "            progress_bar += f\"Latest: {float(next_y):.4f}, SoFarBest: {(inputs['y_vals'].max()):.4f}\"\n",
        "            sys.stdout.write(progress_bar)\n",
        "            sys.stdout.flush()\n",
        "            if t == n_eval:\n",
        "                print()\n",
        "    result = inputs['y_vals'][-1]\n",
        "    sofar_best = (inputs['y_vals']).max()\n",
        "    return result, sofar_best, inputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xjou5N1V1Si"
      },
      "source": [
        "## 2. sklearn 2nd order polynomial regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqAHNmVaJS90"
      },
      "source": [
        "def LocalSearch_PolyReg(args, inputs_, evaluate, tqdm_on=False, progress_on=False, back_to_best=True, ascent_trials=10, pre_calc_acq=False):\n",
        "    n_eval = args.n_eval\n",
        "    N = args.N\n",
        "\n",
        "    # copy inputs (containing initial state)\n",
        "    inputs = {k:v for k, v in inputs_.items()}\n",
        "    n_init = inputs['x_vals'].shape[0]\n",
        "\n",
        "    # Produce more initial states, by randomly walking\n",
        "    next_x = inputs['x_vals'][-1].copy()\n",
        "    for t in range(n_init, args.n_init):\n",
        "        flip_ind = random_next(inputs, next_x)\n",
        "        next_x = flip(next_x.reshape(-1), flip_ind)\n",
        "        next_y = evaluate(next_x)\n",
        "        add_data(inputs, next_x, next_y)\n",
        "\n",
        "    # progress bar\n",
        "    if progress_on:\n",
        "        print(\"Local Search with Polynomial Regression\")\n",
        "        PROG_LEN = args.terminal_size \n",
        "        ratio = (args.n_init-1) / n_eval\n",
        "        progressed_len = int(PROG_LEN * ratio)\n",
        "        progress_bar = f'Flip {args.n_init-1}/{n_eval} ({100.*ratio:.1f}%)' + '|' + '#'*progressed_len + '-'*(PROG_LEN-progressed_len) + '| '\n",
        "        progress_bar += f\"Latest: {inputs['y_vals'][-1]:.4f}, SoFarBest: {(inputs['y_vals'].min()):.4f}\"\n",
        "        sys.stdout.write(progress_bar)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    # Train initial statistical model\n",
        "\n",
        "    LR = Pipeline([('poly', PolynomialFeatures(interaction_only=True)),\n",
        "                  ('linear', LinearRegression() )])\n",
        "    LR.fit(inputs['x_vals'], inputs['y_vals'])\n",
        "\n",
        "    PATH = None\n",
        "    for t in range(args.n_init, n_eval+1):\n",
        "        # flip index suggestion and move a step\n",
        "        if PATH is None:\n",
        "            stat_model = lambda x: LR.predict(x.reshape(1,N))[0]\n",
        "            if pre_calc_acq:\n",
        "                acqs_ = LR.predict(states)\n",
        "                flip_ind = greedy_neighbor_suggestion(args, stat_model, inputs, n_eval - t + 1, tqdm_on=tqdm_on, trials=ascent_trials, calculated_acqs=acqs_)\n",
        "            else:\n",
        "                flip_ind = greedy_neighbor_suggestion(args, stat_model, inputs, n_eval - t + 1, tqdm_on=tqdm_on, trials=ascent_trials)\n",
        "            next_x = flip(next_x.reshape(-1), flip_ind)\n",
        "            next_y = evaluate(next_x)\n",
        "            add_data(inputs, next_x, next_y)\n",
        "        elif back_to_best:\n",
        "            left_chance = n_eval - t + 1\n",
        "            flip_ind = PATH[-left_chance]\n",
        "            next_x = flip(next_x.reshape(-1), flip_ind)\n",
        "            next_y = evaluate(next_x)\n",
        "            add_data(inputs, next_x, next_y)\n",
        "            if left_chance-1>0 and (left_chance-1) % 2 == 0 and next_y > objective_y:  # new reachable best --> update PATH\n",
        "                PATH[-(left_chance-1):] = wander(next_x, left_chance-1)\n",
        "        # train_surrogate model\n",
        "        LR.fit(inputs['x_vals'], inputs['y_vals'])\n",
        "\n",
        "        if back_to_best and (PATH is None) and t >= n_eval - N:  # N is maximum possible hamming distance\n",
        "            PATH, objective_y = monitor_reachable_best(inputs, t, n_eval)\n",
        "        \n",
        "        if progress_on:\n",
        "            if progress_bar is not None:\n",
        "                sys.stdout.write('\\b' * (len(progress_bar)))\n",
        "            ratio = t / n_eval\n",
        "            progressed_len = int(PROG_LEN * ratio)\n",
        "            progress_bar = f'Flip {t}/{n_eval} ({100.*ratio:.1f}%)' + '|' + '#'*progressed_len + '-'*(PROG_LEN-progressed_len) + '| '\n",
        "            progress_bar += f\"Latest: {float(next_y):.4f}, SoFarBest: {(inputs['y_vals'].max()):.4f}\"\n",
        "            sys.stdout.write(progress_bar)\n",
        "            sys.stdout.flush()\n",
        "            if t == n_eval:\n",
        "                print()\n",
        "    result = inputs['y_vals'][-1]\n",
        "    sofar_best = (inputs['y_vals']).max()\n",
        "    return result, sofar_best, inputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsvH07FzM5kH"
      },
      "source": [
        "## Experiment : Comparison btw altorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7VKHNH8RUFr"
      },
      "source": [
        "def Run(args, im_seed_num=None, ctrbs_seed_num=None, start_from_bottom=False, show_seed=True, show_landscape=False, progress_on=True, back_to_best=True):\n",
        "    #random.seed(args.random_seed)\n",
        "    #np.random.seed(args.random_seed)\n",
        "\n",
        "    nkmodel = Construct_NKmodel(args, im_seed_num=im_seed_num, ctrbs_seed_num=ctrbs_seed_num, verbose=show_seed)\n",
        "    if show_landscape:\n",
        "        nkmodel.print_info()\n",
        "    def evaluate(x):\n",
        "        return nkmodel.evaluate(x)  # Minimization Problem: negative=True\n",
        "\n",
        "    if start_from_bottom:\n",
        "        _, anti_opt_state = nkmodel.get_global_optimum(anti_opt=True)\n",
        "        _state_tuple = random.choice(anti_opt_state)\n",
        "        init_x = np.array(_state_tuple)\n",
        "    else:\n",
        "        init_x = np.random.choice(range(args.A), size=args.N)  \n",
        "    init_y = evaluate(init_x)\n",
        "    inputs = {'x_vals': init_x.reshape((1,-1)), 'y_vals': init_y}\n",
        "\n",
        "    # Produce more initial states, by randomly walking from init_x\n",
        "    next_x = init_x.copy()\n",
        "    for t in range(1, args.n_init):\n",
        "        flip_ind = random_next(inputs, next_x)\n",
        "        next_x = flip(next_x.reshape(-1), flip_ind)\n",
        "        next_y = evaluate(next_x)\n",
        "        add_data(inputs, next_x, next_y)\n",
        "    print(inputs)\n",
        "\n",
        "    print(\"Comparison started\")\n",
        "\n",
        "    rw = EasyDict({})\n",
        "    t = time.time()\n",
        "    rw.finals, rw.sofars, rw.inp = Random_Walk(args, inputs, evaluate, progress_on=progress_on, back_to_best=back_to_best)\n",
        "    print(f\"Random Walk runtime: {time.time()-t:.4f}sec\")\n",
        "    print()\n",
        "    bo = EasyDict({})\n",
        "    t = time.time()\n",
        "    bo.finals, bo.sofars, bo.inp = BOCS_loc(args, inputs, evaluate, progress_on=progress_on, back_to_best=back_to_best, ascent_trials=10, pre_calc_acq=True)\n",
        "    print(f\"BOCS_LOC runtime: {time.time()-t:.4f}sec\")\n",
        "    print()\n",
        "    pr = EasyDict({})\n",
        "    t = time.time()\n",
        "    pr.finals, pr.sofars, pr.inp = LocalSearch_PolyReg(args, inputs, evaluate, progress_on=progress_on, back_to_best=back_to_best, ascent_trials=10, pre_calc_acq=True)\n",
        "    print(f\"POLY_REG runtime: {time.time()-t:.4f}sec\")\n",
        "\n",
        "    optlist = nkmodel.get_optimum_and_more(10)\n",
        "    reachable_best = None\n",
        "    for i in range(10):\n",
        "        opt, optstates = optlist[i][\"fitness\"], optlist[i][\"states\"]\n",
        "        print(f\"{i+1}-th optimum: {opt} {optstates}\")\n",
        "        if reachable_best is None and any([is_reachable(init_x, np.array(st), args.n_eval) for st in optstates]):\n",
        "            reachable_best = opt \n",
        "    print()\n",
        "\n",
        "    return rw, bo, pr, reachable_best, optlist[0][\"fitness\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJlsiUYCNwxN"
      },
      "source": [
        "do_experiment = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrEZyTRO2Dbp"
      },
      "source": [
        "loss_tt = EasyDict({'randomwalk':[], 'bocsloc':[], 'polyreg':[]})\n",
        "ys_tt = EasyDict({'randomwalk':[], 'bocsloc':[], 'polyreg':[]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqCnuESdr0Zb"
      },
      "source": [
        "# stochastic ascent score: last acq\n",
        "# greedy search ver 2. --> visited penalty applied \n",
        "# back to reachable best true\n",
        "\n",
        "if do_experiment:\n",
        "    random.seed(args.random_seed)\n",
        "    np.random.seed(args.random_seed)\n",
        "\n",
        "    iter = [(im, ctrbs, i_) for im in range(3) for ctrbs in range(10) for i_ in range(10)]\n",
        "    resume = 0\n",
        "    for i in range(resume, len(iter)):\n",
        "        im, ctrbs, _ = iter[i]\n",
        "        try:\n",
        "            rw, bo, pr, r_best, glob_opt = Run(args, im_seed_num=im, ctrbs_seed_num=ctrbs, back_to_best=True)\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt(f'stopped at iter[{i}]')\n",
        "        except TimeoutError:\n",
        "            continue\n",
        "        except Exception:\n",
        "            raise Exception(f'stopped at iter[{i}]')\n",
        "        loss_tt.randomwalk.append(r_best - rw.finals)\n",
        "        loss_tt.bocsloc.append(r_best - bo.finals)\n",
        "        loss_tt.polyreg.append(r_best - pr.finals)\n",
        "        ys_tt.randomwalk.append(rw.inp['y_vals'])\n",
        "        ys_tt.bocsloc.append(bo.inp['y_vals'])\n",
        "        ys_tt.polyreg.append(pr.inp['y_vals'])\n",
        "\n",
        "        print()\n",
        "\n",
        "    print(\"Random Walk; Loss = (reachable best) - (final score) = \", np.array(loss_tt.randomwalk).mean(), \", std\", np.array(loss_tt.randomwalk).std())\n",
        "    print(\"BOCS_LOC;    Loss = (reachable best) - (final score) = \", np.array(loss_tt.bocsloc).mean(), \", std\", np.array(loss_tt.bocsloc).std())\n",
        "    print(\"Poly_Regr. ; Loss = (reachable best) - (final score) = \", np.array(loss_tt.polyreg).mean(), \", std\", np.array(loss_tt.polyreg).std())\n",
        "\n",
        "    print(\"Random Walk; averaged path\", np.stack(ys_tt.randomwalk).mean(axis=0))\n",
        "    print(\"BOCS_LOC   ; averaged path\", np.stack(ys_tt.bocsloc).mean(axis=0))\n",
        "    print(\"Poly_Regr. ; averaged path\", np.stack(ys_tt.polyreg).mean(axis=0))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLIQ__Dy2Hol"
      },
      "source": [
        "loss_ff = EasyDict({'randomwalk':[], 'bocsloc':[], 'polyreg':[]})\n",
        "ys_ff = EasyDict({'randomwalk':[], 'bocsloc':[], 'polyreg':[]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydp_yRNucgtK"
      },
      "source": [
        "# stochastic ascent score: last acq\n",
        "# greedy search ver 2. --> visited penalty applied again\n",
        "# back to reachable best false\n",
        "\n",
        "if do_experiment:\n",
        "    random.seed(args.random_seed)\n",
        "    np.random.seed(args.random_seed)\n",
        "\n",
        "    iter = [(im, ctrbs, i_) for im in range(3) for ctrbs in range(10) for i_ in range(10)]\n",
        "    resume = 0\n",
        "    for i in range(resume, len(iter)):\n",
        "        im, ctrbs, _ = iter[i]\n",
        "        try:\n",
        "            rw, bo, pr, r_best, glob_opt = Run(args, im_seed_num=im, ctrbs_seed_num=ctrbs, back_to_best=False)\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt(f'stopped at iter[{i}]')\n",
        "        except TimeoutError:\n",
        "            continue\n",
        "        except Exception:\n",
        "            raise Exception(f'stopped at iter[{i}]')\n",
        "        loss_ff.randomwalk.append(r_best - rw.finals)\n",
        "        loss_ff.bocsloc.append(r_best - bo.finals)\n",
        "        loss_ff.polyreg.append(r_best - pr.finals)\n",
        "        ys_ff.randomwalk.append(rw.inp['y_vals'])\n",
        "        ys_ff.bocsloc.append(bo.inp['y_vals'])\n",
        "        ys_ff.polyreg.append(pr.inp['y_vals'])\n",
        "        print()\n",
        "    print(\"Random Walk; Loss = (reachable best) - (final score) = \", np.array(loss_ff.randomwalk).mean(), \", std\", np.array(loss_ff.randomwalk).std())\n",
        "    print(\"BOCS_LOC;    Loss = (reachable best) - (final score) = \", np.array(loss_ff.bocsloc).mean(), \", std\", np.array(loss_ff.bocsloc).std())\n",
        "    print(\"Poly_Regr. ; Loss = (reachable best) - (final score) = \", np.array(loss_ff.polyreg).mean(), \", std\", np.array(loss_ff.polyreg).std())\n",
        "\n",
        "    print(\"Random Walk; averaged path\", np.stack(ys_ff.randomwalk).mean(axis=0))\n",
        "    print(\"BOCS_LOC   ; averaged path\", np.stack(ys_ff.bocsloc).mean(axis=0))\n",
        "    print(\"Poly_Regr. ; averaged path\", np.stack(ys_ff.polyreg).mean(axis=0))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ieiw8n82NU1"
      },
      "source": [
        "loss_sfbf = EasyDict({'randomwalk':[], 'bocsloc':[], 'polyreg':[]})\n",
        "ys_sfbf = EasyDict({'randomwalk':[], 'bocsloc':[], 'polyreg':[]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYiVAz3yLu4n"
      },
      "source": [
        "# stochastic ascent score: last acq\n",
        "# greedy search ver 2. --> visited penalty applied again\n",
        "# back to reachable best True\n",
        "# start from bottom\n",
        "\n",
        "if do_experiment:\n",
        "    random.seed(args.random_seed)\n",
        "    np.random.seed(args.random_seed)\n",
        "\n",
        "    iter = [(im, ctrbs, i_) for im in range(3) for ctrbs in range(10) for i_ in range(10)]\n",
        "    resume = 0\n",
        "    for i in range(resume, len(iter)):\n",
        "        im, ctrbs, _ = iter[i]\n",
        "        try:\n",
        "            rw, bo, pr, r_best, glob_opt = Run(args, im_seed_num=im, ctrbs_seed_num=ctrbs, back_to_best=True, start_from_bottom=True)\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt(f'stopped at iter[{i}]')\n",
        "        except TimeoutError:\n",
        "            continue\n",
        "        except Exception:\n",
        "            raise Exception(f'stopped at iter[{i}]')\n",
        "        loss_sfbf.randomwalk.append(r_best - rw.finals)\n",
        "        loss_sfbf.bocsloc.append(r_best - bo.finals)\n",
        "        loss_sfbf.polyreg.append(r_best - pr.finals)\n",
        "        ys_sfbf.randomwalk.append(rw.inp['y_vals'])\n",
        "        ys_sfbf.bocsloc.append(bo.inp['y_vals'])\n",
        "        ys_sfbf.polyreg.append(pr.inp['y_vals'])\n",
        "        print()\n",
        "    print(\"Random Walk; Loss = (reachable best) - (final score) = \", np.array(loss_sfbf.randomwalk).mean(), \", std\", np.array(loss_sfbf.randomwalk).std())\n",
        "    print(\"BOCS_LOC;    Loss = (reachable best) - (final score) = \", np.array(loss_sfbf.bocsloc).mean(), \", std\", np.array(loss_sfbf.bocsloc).std())\n",
        "    print(\"Poly_Regr. ; Loss = (reachable best) - (final score) = \", np.array(loss_sfbf.polyreg).mean(), \", std\", np.array(loss_sfbf.polyreg).std())\n",
        "\n",
        "    print(\"Random Walk; averaged path\", np.stack(ys_sfbf.randomwalk).mean(axis=0))\n",
        "    print(\"BOCS_LOC   ; averaged path\", np.stack(ys_sfbf.bocsloc).mean(axis=0))\n",
        "    print(\"Poly_Regr. ; averaged path\", np.stack(ys_sfbf.polyreg).mean(axis=0))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PHR2pLkXBKS"
      },
      "source": [
        "loss_sfb = EasyDict({'randomwalk':[], 'bocsloc':[], 'polyreg':[]})\n",
        "ys_sfb = EasyDict({'randomwalk':[], 'bocsloc':[], 'polyreg':[]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfXdVj2q22fH"
      },
      "source": [
        "# stochastic ascent score: last acq\n",
        "# greedy search --> visited penalty applied again\n",
        "# back to reachable best False\n",
        "# start from bottom\n",
        "\n",
        "if do_experiment:\n",
        "    random.seed(args.random_seed)\n",
        "    np.random.seed(args.random_seed)\n",
        "\n",
        "    iter = [(im, ctrbs, i_) for im in range(3) for ctrbs in range(10) for i_ in range(10)]\n",
        "    resume = 0\n",
        "    for i in range(resume, len(iter)):\n",
        "        im, ctrbs, _ = iter[i]\n",
        "        try:\n",
        "            rw, bo, pr, r_best, glob_opt = Run(args, im_seed_num=im, ctrbs_seed_num=ctrbs, back_to_best=False, start_from_bottom=True)\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt(f'stopped at iter[{i}]')\n",
        "        except TimeoutError:\n",
        "            continue\n",
        "        except Exception:\n",
        "            raise Exception(f'stopped at iter[{i}]')\n",
        "        loss_sfb.randomwalk.append(r_best - rw.finals)\n",
        "        loss_sfb.bocsloc.append(r_best - bo.finals)\n",
        "        loss_sfb.polyreg.append(r_best - pr.finals)\n",
        "        ys_sfb.randomwalk.append(rw.inp['y_vals'])\n",
        "        ys_sfb.bocsloc.append(bo.inp['y_vals'])\n",
        "        ys_sfb.polyreg.append(pr.inp['y_vals'])\n",
        "        print()\n",
        "    print(\"Random Walk; Loss = (reachable best) - (final score) = \", np.array(loss_sfb.randomwalk).mean(), \", std\", np.array(loss_sfb.randomwalk).std())\n",
        "    print(\"BOCS_LOC;    Loss = (reachable best) - (final score) = \", np.array(loss_sfb.bocsloc).mean(), \", std\", np.array(loss_sfb.bocsloc).std())\n",
        "    print(\"Poly_Regr. ; Loss = (reachable best) - (final score) = \", np.array(loss_sfb.polyreg).mean(), \", std\", np.array(loss_sfb.polyreg).std())\n",
        "\n",
        "    print(\"Random Walk; averaged path\", np.stack(ys_sfb.randomwalk).mean(axis=0))\n",
        "    print(\"BOCS_LOC   ; averaged path\", np.stack(ys_sfb.bocsloc).mean(axis=0))\n",
        "    print(\"Poly_Regr. ; averaged path\", np.stack(ys_sfb.polyreg).mean(axis=0))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX6WdT1Bw5zk"
      },
      "source": [
        "## Experiment : Comparison btw 'trial' in 'greedy neighbor suggestion'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S_pimBre6s3"
      },
      "source": [
        "def Compare_BOCS_trial(args, im_seed_num=None, ctrbs_seed_num=None, start_from_bottom=False, show_seed=True, show_landscape=False, progress_on=True, back_to_best=True):\n",
        "    #random.seed(args.random_seed)\n",
        "    #np.random.seed(args.random_seed)\n",
        "\n",
        "    nkmodel = Construct_NKmodel(args, im_seed_num=im_seed_num, ctrbs_seed_num=ctrbs_seed_num, verbose=show_seed)\n",
        "    if show_landscape:\n",
        "        nkmodel.print_info()\n",
        "    def evaluate(x):\n",
        "        return nkmodel.evaluate(x)  # Minimization Problem: negative=True\n",
        "\n",
        "    if start_from_bottom:\n",
        "        _, anti_opt_state = nkmodel.get_global_optimum(anti_opt=True)\n",
        "        _state_tuple = random.choice(anti_opt_state)\n",
        "        init_x = np.array(_state_tuple)\n",
        "    else:\n",
        "        init_x = np.random.choice(range(args.A), size=args.N)  \n",
        "    init_y = evaluate(init_x)\n",
        "    inputs = {'x_vals': init_x.reshape(1,-1), 'y_vals': init_y}\n",
        "\n",
        "    # Produce more initial states, by randomly walking from init_x\n",
        "    next_x = init_x.copy()\n",
        "    for t in range(1, args.n_init):\n",
        "        flip_ind = random_next(inputs, next_x)\n",
        "        next_x = flip(next_x.reshape(-1), flip_ind)\n",
        "        next_y = evaluate(next_x)\n",
        "        add_data(inputs, next_x, next_y)\n",
        "    print(inputs)\n",
        "\n",
        "    print(\"Comparison started\")\n",
        "    \n",
        "    rw = EasyDict({})\n",
        "    t = time.time()\n",
        "    rw.finals, rw.sofars, rw.inp = Random_Walk(args, inputs, evaluate, progress_on=progress_on, back_to_best=back_to_best)\n",
        "    print(f\"Random Walk runtime: {time.time()-t:.4f}sec\")\n",
        "    print()\n",
        "\n",
        "    bo_list = []\n",
        "    for trial_exp in range(10):\n",
        "        bo = EasyDict({})\n",
        "        t = time.time()\n",
        "        bo.finals, bo.sofars, bo.inp = BOCS_loc(args, inputs, evaluate, progress_on=progress_on, back_to_best=back_to_best, ascent_trials=(2**trial_exp), pre_calc_acq=True)\n",
        "        bo.t_gap = time.time()-t\n",
        "        print(f\"BOCS_LOC (ascent trial {(2**trial_exp)}) runtime: {bo.t_gap:.4f}sec\")\n",
        "        bo_list.append(bo)\n",
        "        print()\n",
        "    \n",
        "    optlist = nkmodel.get_optimum_and_more(10)\n",
        "    reachable_best = None\n",
        "    for i in range(10):\n",
        "        opt, optstates = optlist[i][\"fitness\"], optlist[i][\"states\"]\n",
        "        print(f\"{i+1}-th optimum: {opt} {optstates}\")\n",
        "        if reachable_best is None and any([is_reachable(init_x, np.array(st), args.n_eval) for st in optstates]):\n",
        "            reachable_best = opt \n",
        "    print()\n",
        "\n",
        "    return rw, bo_list, reachable_best, optlist[0][\"fitness\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIIG2qlW2gfL"
      },
      "source": [
        "rw_loss = []\n",
        "bo_compares = [EasyDict({'trials':2**t,'loss':[], 'time':[]}) for t in range(10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMJqovtwfPHH"
      },
      "source": [
        "# stochastic ascent score: last acq\n",
        "# greedy search --> visited penalty applied again\n",
        "# back to reachable best True\n",
        "# start from bottom\n",
        "\n",
        "if do_experiment:\n",
        "    random.seed(args.random_seed)\n",
        "    np.random.seed(args.random_seed)\n",
        "\n",
        "    iter = [(im, ctrbs, i_) for im in range(3) for ctrbs in range(5) for i_ in range(5)]\n",
        "    resume = 0\n",
        "    for i in range(resume, len(iter)):\n",
        "        im, ctrbs, _ = iter[i]\n",
        "        try:\n",
        "            rw, bo_list, r_best, glob_opt = Compare_BOCS_trial(args, im_seed_num=im, ctrbs_seed_num=ctrbs, back_to_best=True, start_from_bottom=True)\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt(f'stopped at iter[{i}]')\n",
        "        except TimeoutError:\n",
        "            continue\n",
        "        except Exception:\n",
        "            raise Exception(f'stopped at iter[{i}]')\n",
        "        rw_loss.append(r_best - rw.finals)\n",
        "        for t in range(10):\n",
        "            assert bo_compares[t].trials == 2**t\n",
        "            bo_compares[t].loss.append(r_best - bo_list[t].finals)\n",
        "            bo_compares[t].time.append(bo_list[t].t_gap)\n",
        "        print()\n",
        "    \n",
        "    print(\"Random Walk:\\n    Loss = (reachable best) - (final score) = \", np.array(rw_loss).mean(), \", std\", np.array(rw_loss).std())\n",
        "    print()\n",
        "    for t in range(10):\n",
        "        assert bo_compares[t].trials == 2**t\n",
        "        print(f\"BOCS LOC ascentTrial{2**t}:\\n    Loss = (reachable best) - (final score) = \", np.array(bo_compares[t].loss).mean(), \", std\", np.array(bo_compares[t].loss).std())\n",
        "        print(f\"BOCS LOC ascentTrial{2**t}: average time is \", np.array(bo_compares[t].time).mean())\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJqZjmQ-Ixp0"
      },
      "source": [
        "# BOCS with Hoo-man"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8ohWTnfkf9V"
      },
      "source": [
        "def _show_best(best):\n",
        "    print(\"*** Displaying Your Best Attempt So Far ***\")\n",
        "    for key in best:\n",
        "        print(\"best\", key, \":\", best[key])\n",
        "\n",
        "def _show_all(inputs):\n",
        "    print(\"*** Displaying All Your Attempt So Far (state --> score) ***\")\n",
        "    x_vals, y_vals = inputs['x_vals'], inputs['y_vals']\n",
        "    for i in range(y_vals.size):\n",
        "        print(f\"Round{i:02d}: {x_vals[i]} --> {y_vals[i]:.4f}\")\n",
        "\n",
        "def _yes_or_no():\n",
        "    \"\"\"\n",
        "    example:\n",
        "    if _yes_or_no():\n",
        "        pass\n",
        "    \"\"\"\n",
        "    YORN = 'undefined'\n",
        "    while YORN.lower()[0] not in ['y', 'n']:\n",
        "        YORN = input(\"yes[y] or no[n]: \").strip()\n",
        "    return YORN.lower()[0] == 'y'\n",
        "\n",
        "\n",
        "def GAME(args, chances=None, can_restart=True):\n",
        "\n",
        "    print(\"Preparing the game.....\")\n",
        "\n",
        "    N = args.N\n",
        "    if chances is None:\n",
        "        chances = args.n_eval\n",
        "    elif chances != args.n_eval:\n",
        "        print(\"args.n_eval changed\")\n",
        "        args.n_eval = chances\n",
        "    nkmodel = Construct_NKmodel(args)\n",
        "    optimum, _ = nkmodel.get_global_optimum()\n",
        "    optlist = nkmodel.get_optimum_and_more(2**N)\n",
        "    #print(nkmodel.interdependence)\n",
        "    \n",
        "    start = True\n",
        "    while start:\n",
        "        print(f\"\\nNK MODEL GAME MODE ON\\n\")\n",
        "        print(f\"Before start, write {N} numbers: each number should be 0 or 1.\")\n",
        "        print(f\"Separate numbers by ' '(spacebar).\")\n",
        "        INPUT = []\n",
        "        error_str=None\n",
        "        while len(INPUT) != N or not set(INPUT).issubset(set(['0', '1'])):\n",
        "            INPUT = input(\"* Your input: \").strip().split()\n",
        "            if len(INPUT) != N:\n",
        "                print(f\"Wrong length: write {N} numbers and separate with spacebars.\")\n",
        "            if not set(INPUT).issubset(set(['0', '1'])):\n",
        "                print(\"Wrong number: write 0 or 1 only\")\n",
        "        init_x = np.array([int(x) for x in INPUT])\n",
        "        init_y = nkmodel.evaluate(init_x)\n",
        "        inputs = {'x_vals': init_x.reshape(1,-1), 'y_vals': init_y}\n",
        "        \n",
        "        best = {'x': init_x.copy(), 'y': float(init_y), 'round': 1}\n",
        "        \n",
        "        # Surrogate model\n",
        "        LR = LinReg(nVars=N, order=2)\n",
        "\n",
        "        next_x = init_x.copy()\n",
        "        next_y = init_y\n",
        "        fit_diff = 0\n",
        "        ctrbs_diff = np.zeros(N)\n",
        "        back_to_best_flag = False\n",
        "        PATH = None\n",
        "        print()\n",
        "        for ROUND in range(1, chances+1):\n",
        "            print(f\"*** Round {ROUND}/{chances} ***\")\n",
        "            print( \"Previous results:\")\n",
        "            print( \"Current state:\", next_x, '<== IMPROVED' if ROUND>1 and np.all(best['x'] == next_x) else '')\n",
        "            print(f\"Current fitness: {float(next_y):.4f} ({fit_diff:.4f} from before)\")\n",
        "            print( \"Current improvement of contributions:\", ctrbs_diff)\n",
        "            print()\n",
        "\n",
        "            # flip index suggestion\n",
        "            left_chance = chances - ROUND + 1\n",
        "            if PATH is None:\n",
        "                if ROUND > 1:\n",
        "                    stat_model = lambda x: LR.surrogate_model(x, LR.alpha)\n",
        "                    acqs_ = stat_model(states)\n",
        "                    flip_suggest = greedy_neighbor_suggestion(args, stat_model, inputs, left_chance, tqdm_on=False, calculated_acqs=acqs_)\n",
        "                else:\n",
        "                    flip_suggest = np.random.randint(N) # random suggestion\n",
        "            else:\n",
        "                flip_suggest = PATH[-left_chance]\n",
        "            \n",
        "            print(f\"* The ALGORITHM SUGGESTED to flip <{flip_suggest+1}>-th digit. *\")\n",
        "            # Flip-index Choice\n",
        "            print( \"NOW: Which digit do YOU want to flip?\")\n",
        "            print(f\"     Write a number m if you want to flip m-th digit: (1 <= m <= {N})\")\n",
        "            print(f\"     Or, write 'BEST' if you want to display the BEST results so far\")\n",
        "            print(f\"     Or, write 'history' if you want to display the ALL your attempts so far\")\n",
        "            confirm = False\n",
        "            while not confirm:\n",
        "                flip_idx = None\n",
        "                while flip_idx not in range(1, N+1):\n",
        "                    INPUT = input(\"* Your input: \").strip()\n",
        "                    if INPUT == 'BEST':\n",
        "                        _show_best(best)\n",
        "                    elif INPUT == 'history':\n",
        "                        _show_all(inputs)\n",
        "                    elif INPUT.isdigit():\n",
        "                        flip_idx = int(INPUT)\n",
        "                        if flip_idx not in range(1, N+1):\n",
        "                            print(f\"Wrong number: Write a number from 1 to {N}: \")\n",
        "                    else:\n",
        "                        print(\"Wrong input format: Write again: \")\n",
        "                flip_idx -= 1  # index: 0 ~ N-1\n",
        "                print(\"Do you really want to change the state as follows?:\")\n",
        "                print(next_x, \"-->\", flip(next_x, flip_idx))\n",
        "                if _yes_or_no():\n",
        "                    confirm = True\n",
        "                    # If the player did different action from the suggestion, initialize PATH\n",
        "                    if flip_suggest != flip_idx:\n",
        "                        PATH = None \n",
        "                    # Compute next state / fitness / contribution improvement\n",
        "                    next_y_temp, ctrbs_diff = nkmodel.fitness_and_contrib_diff(next_x, flip_idx)\n",
        "                    next_x = flip(next_x, flip_idx)\n",
        "                    fit_diff = next_y_temp - float(next_y)\n",
        "                    next_y = np.array([next_y_temp])\n",
        "                else:\n",
        "                    print(\"You answered NO: Re-write your input. \")\n",
        "                    print( \"NOW: Which digit do U wanna flip?\")\n",
        "                    print(f\"     Write a number m if you want to flip m-th digit: (1 <= m <= {N})\")\n",
        "                    print(f\"     Or, Write 'BEST' if you wnat to display the BEST results so far\")\n",
        "            \n",
        "            # inputs update\n",
        "            add_data(inputs, next_x, next_y)\n",
        "\n",
        "            # Possibly, update PATH (if it exists)\n",
        "            if (PATH is not None) and (left_chance-1>0) and ((left_chance-1) % 2 == 0) and (next_y > objective_y):  # new reachable best --> update PATH\n",
        "                PATH[-(left_chance-1):] = wander(next_x, left_chance-1)\n",
        "            \n",
        "            # At some point, we should save a PATH to reachable best so far.\n",
        "            if (PATH is None) and ROUND >= chances - N:  # N is maximum possible hamming distance\n",
        "                PATH, objective_y = monitor_reachable_best(inputs, ROUND, chances)\n",
        "            \n",
        "            # surrogate model train\n",
        "            LR.train(inputs)\n",
        "\n",
        "            # best state update\n",
        "            if next_y > best['y']:\n",
        "                best[\"y\"] = next_y\n",
        "                best[\"x\"] = next_x\n",
        "                best[\"round\"] = ROUND\n",
        "            print()\n",
        "            print()\n",
        "        \n",
        "        time.sleep(2)\n",
        "        print(\"THE END: All the chances Ran out!\")\n",
        "        print(\"Finally, your best attempt is:\")\n",
        "        _show_best(best)\n",
        "        print()\n",
        "\n",
        "        print(\"Also, your FINAL attempt is:\")\n",
        "        print(\"*** Displaying Your Final Score ***\")\n",
        "        print(\"final x :\", next_x)\n",
        "        print(\"final y :\", next_y)\n",
        "        print()\n",
        "\n",
        "        # Compute reachable best\n",
        "        reachable_best = None\n",
        "        for ind in range(len(optlist)):\n",
        "            opt, optstates = optlist[ind][\"fitness\"], optlist[ind][\"states\"]\n",
        "            for st in optstates:\n",
        "                if is_reachable(init_x, np.array(st), chances):\n",
        "                    reachable_best = opt\n",
        "                    break\n",
        "            if reachable_best is not None:\n",
        "                break\n",
        "\n",
        "        time.sleep(2)\n",
        "        # Assesment of result\n",
        "        if abs(float(next_y) - optimum) <= 1e-5:\n",
        "            print(\"You have reached the global optimum !!! You made it !!!\")\n",
        "            start = False\n",
        "        else:\n",
        "            print(\"You did not reached the global optimum.\")\n",
        "            if abs(float(next_y) - reachable_best) <= 1e-5:\n",
        "                print(\"But you did your best !! (your final answer is reachable-optimum\")\n",
        "                start = False\n",
        "            else:\n",
        "                print(\"Well, everyone has their bad days.\")\n",
        "        print()\n",
        "        \n",
        "        # Ask whether to restart\n",
        "        start = start and can_restart\n",
        "        time.sleep(3)\n",
        "        if start:\n",
        "            print(\"Do you want to try again? (with the same landscape)\")\n",
        "            if _yes_or_no():\n",
        "                print(\"Caution: your best score will be removed from the memory\\n\")\n",
        "            else:\n",
        "                start = False\n",
        "        \n",
        "        # Scoreboard\n",
        "        if not start:\n",
        "            print(\"Do you want to see the score board?\")\n",
        "            if _yes_or_no():\n",
        "                for i in range(2**N):\n",
        "                    opt, optstates = optlist[i][\"fitness\"], optlist[i][\"states\"]\n",
        "                    print(f\"{i+1}-th optimum: {opt} {optstates}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLME01MWidJa"
      },
      "source": [
        "# Play a GAME!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctJfYzToJx5d"
      },
      "source": [
        "GAME(args, can_restart=False, chances=18)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}